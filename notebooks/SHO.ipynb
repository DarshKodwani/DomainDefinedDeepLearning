{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Harmonic oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The physics\n",
    "\n",
    "Lagrangian for simple harmonic oscillator\n",
    "\n",
    "$$\n",
    "L =   \\frac{1}{2} m(\\frac{dx}{dt})^2 - \\frac{1}{2} kx^2 \n",
    "$$\n",
    "\n",
    "Equation of motion (Euler Lagrange Equation) is given below\n",
    "\n",
    "$$\n",
    "m \\dfrac{d^2x}{dt^2} = -kx~.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact solution is given by \n",
    "\n",
    "$$\n",
    "x(t) = A \\cos{\\omega t + \\phi}\n",
    "$$\n",
    "\n",
    "where $$ \\omega = \\sqrt{\\frac{k}{m}} $$. Lets make a quick plot for this with some dummy numbers to see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Example parameters\n",
    "example_amplitude = 1\n",
    "example_phase = 0\n",
    "example_frequency = 3\n",
    "\n",
    "example_times = np.linspace(0, 10, 200)  # Example input\n",
    "\n",
    "\n",
    "def sho(amplitude, frequency, phase, time):\n",
    "    return amplitude * np.cos(frequency * time + phase)\n",
    "\n",
    "\n",
    "shos = sho(example_amplitude, example_frequency, example_phase, example_times)\n",
    "plt.plot(example_times, shos)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend(\n",
    "    [\n",
    "        f\"SHO with amplitude {example_amplitude}, frequency {example_frequency}, phase {example_phase}\"\n",
    "    ],\n",
    "    loc=\"upper right\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with a NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to fit a standard Neural network to some training data we get from our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create some training data from the known analytic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Select some physics parameters\n",
    "amplitude = 1\n",
    "frequency = 30\n",
    "phase = 0\n",
    "\n",
    "# Create synethtic data\n",
    "time = torch.linspace(0, 1, 200).view(-1, 1)\n",
    "sho_analytic_solution = sho(amplitude, frequency, phase, time).view(-1, 1)\n",
    "\n",
    "# Create a subsample of the data for training\n",
    "time_samples = time[0:50:5]\n",
    "sho_samples = sho_analytic_solution[0:50:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a standard Feed Forward Fully Connected Nueral network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the solution, let's try to design a simple neural network to fit the data. We will use a simple feed forward neural network with 2 hidden layers. The input to the network will be time and the output will be the position of the oscillator at that time. Therefore, the input layer will have 1 neuron and the output layer will have 1 neuron. The number of neurons in the hidden layer is a hyperparameter that we can tune. We will use the Tanh activation function for the hidden layers and the linear activation function for the output layer. We will use the mean squared error loss function and the Adam optimizer for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shoNN(torch.nn.Module):\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        \"\"\"\n",
    "        TODO: Define the input layer, hidden layers, and output layer.\n",
    "        The input layer should be a linear layer followed by an activation function.\n",
    "        The hidden layers should be a sequence of linear layers followed by activation functions.\n",
    "        The output layer should be a linear layer.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the layers here\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement the forward pass.\n",
    "        The input should pass through the input layer, the hidden layers, and the output layer in order.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass here\n",
    "\n",
    "        pass\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the shoNN class\n",
    "test_model = shoNN(1, 1, 20, 2)\n",
    "\n",
    "# Set the weights of the model for testing\n",
    "with torch.no_grad():\n",
    "    for param in test_model.parameters():\n",
    "        param.fill_(0.5)\n",
    "\n",
    "assert isinstance(\n",
    "    test_model, torch.nn.Module\n",
    "), \"The shoNN class should inherit from torch.nn.Module.\"\n",
    "assert hasattr(test_model, \"forward\"), \"The shoNN class should have a 'forward' method.\"\n",
    "\n",
    "# Test a forward pass\n",
    "input_data = torch.tensor([[0.0], [0.1], [0.2], [0.3]])\n",
    "output_data = test_model(input_data)\n",
    "\n",
    "assert output_data.shape == (4, 1), \"The output shape is incorrect.\"\n",
    "\n",
    "# Expected output\n",
    "expected_output = torch.tensor([[10.499288], [10.499669], [10.499841], [10.499922]])\n",
    "\n",
    "assert torch.allclose(\n",
    "    output_data, expected_output, atol=1e-6\n",
    "), \"The output values are not as expected.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the NN, lets visualise what we are about to do. The graph below shows the analytic solution in blue, which is an exact solution to the differential equation for a SHO. We have then sampled a subset of the solutions to the equation at specific times to create a training 'y' variable, and will be fed to the NN for training. \n",
    "\n",
    "Note that the training data onll covers a small subset of the full range of 'x' values. This is important as it highlights that we are not going to simply fit the NN to the functional data, rather a subset of the data and 'learn' the rest of the solution by providing it 'physical insight' directly into the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(time, sho_analytic_solution, label=\"Exact solution\")\n",
    "plt.scatter(time_samples, sho_samples, color=\"tab:orange\", label=\"Training data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create some plotting functions for easy visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def plot_result(x, y, x_data, y_data, yh, xp=None):\n",
    "    \"Pretty plot training results\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y, color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n",
    "    plt.plot(\n",
    "        x,\n",
    "        yh,\n",
    "        color=\"tab:blue\",\n",
    "        linewidth=4,\n",
    "        alpha=0.8,\n",
    "        label=\"Neural network prediction\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        x_data, y_data, s=60, color=\"tab:orange\", alpha=0.4, label=\"Training data\"\n",
    "    )\n",
    "    if xp is not None:\n",
    "        plt.scatter(\n",
    "            xp,\n",
    "            -0 * torch.ones_like(xp),\n",
    "            s=60,\n",
    "            color=\"tab:green\",\n",
    "            alpha=0.4,\n",
    "            label=\"Physics loss training locations\",\n",
    "        )\n",
    "    l = plt.legend(loc=(1.01, 0.34), frameon=False, fontsize=\"large\")\n",
    "    plt.setp(l.get_texts(), color=\"k\")\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.text(1.065, 0.7, \"Training step: %i\" % (i + 1), fontsize=\"xx-large\", color=\"k\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "def save_gif_PIL(outfile, files, fps=5, loop=0):\n",
    "    images = [Image.open(fn) for fn in files]\n",
    "    images[0].save(\n",
    "        outfile,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        optimize=False,\n",
    "        duration=1000 / fps,\n",
    "        loop=loop,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the NN on training data with a normal MSE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will train the NN with a normal MSE loss function and see how it performs. We will then plot the results to see how well the NN has learned the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mse_loss(predicted: torch.tensor, target: torch.tensor) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    This loss function should return the mean squared error between the predicted and target values.\n",
    "    Note that at this point, we are not including the physics loss.\n",
    "    \"\"\"\n",
    "    predicted = predicted.squeeze()\n",
    "    target = target.squeeze()\n",
    "\n",
    "    loss = # TODO: Implement the loss function here\n",
    "    return loss\n",
    "\n",
    "assert simple_mse_loss(torch.tensor([[1.0], [2.0], [3.0]]), torch.tensor([[1.0], [2.0], [3.0]])).item() == 0.0, \"The loss is not correct.\"\n",
    "assert torch.allclose(simple_mse_loss(torch.tensor([[1.0], [2.0], [3.0]]), torch.tensor([[1.1], [2.1], [3.1]])).detach(), torch.tensor(0.01)), \"The loss is not correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# train standard neural network to fit training data\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Instantiate the model that has one input (time) and one output (the amplitude of the SHO)\n",
    "model = # TODO Instantiate the model here\n",
    "\n",
    "assert isinstance(model, torch.nn.Module), \"model should be an instance of torch.nn.Module\"\n",
    "assert model.input_layer[0].in_features == 1, \"The input dimension of the model should be 1.\"\n",
    "assert model.output_layer.out_features == 1, \"The output dimension of the model should be 1.\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "files = []\n",
    "losses = []\n",
    "for i in range(6000):\n",
    "    optimizer.zero_grad()\n",
    "    sho_predicted = model(time_samples)\n",
    "    loss = simple_mse_loss(predicted=sho_predicted, target=sho_samples)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # plot the result as training progresses\n",
    "    if (i+1) % 10 == 0: \n",
    "        \n",
    "        sho_predicted_plot = model(time).detach()\n",
    "        \n",
    "        plot_result(time,sho_analytic_solution,time_samples,sho_samples,sho_predicted_plot)\n",
    "        \n",
    "        file = \"../plots/nn_%.8i.png\"%(i+1)\n",
    "        plt.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "        files.append(file)\n",
    "    \n",
    "        if (i+1) % 500 == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "save_gif_PIL(\"nn_nosholoss.gif\", files, fps=20, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reflect on these results.\n",
    "1. We see that the NN fits the training data we provided perfectly, which is no surprise as it has more than enough parameters (in fact, in this case more parameters than data points). So the system is completely overfit\n",
    "2. We see that it tries to learn but the loss function platues fairly rapidly with no change with training steps. The final result is a model that simply cannot predict the unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the NN with SHO and boundary term loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SHO loss function is:\n",
    "\n",
    "$$\n",
    "L_{sho} = \\frac{1}{N} \\sum \\left(\\frac{d^2x}{dt^2} +\\omega^2 x \\right)^2\n",
    "$$\n",
    "\n",
    "And the boundary loss is:\n",
    "\n",
    "$$\n",
    "L_{boundary} = (x(t_{initial}) - 1)^2 + \\left( \\frac{dx}{dt}(t_{initial}) \\right)^2\n",
    "$$\n",
    "\n",
    "where $t_{initial} = 0$. This simply says that the initial amplitude is 1 (and hence a cosine) and the initial velocity is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train a NN with these loss terms and various example parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_and_derivatives(model, time_vector):\n",
    "    \"\"\"\n",
    "    This function should return the model outputs and the first and second derivatives of the model outputs with respect to time_vector.\n",
    "\n",
    "    Use the torch.autograd.grad function to compute the derivatives of the model output with respect to time_vector.\n",
    "    Args:\n",
    "        model: The neural network model.\n",
    "        time_vector: The input to the model.\n",
    "    \"\"\"\n",
    "    # Enable gradient computation for the time_vector\n",
    "    time_vector.requires_grad = True\n",
    "\n",
    "    # TODO: Fill in the definition of the model_outputs, first_derivative, and second_derivative\n",
    "\n",
    "    model_outputs = # TODO: Implement the model outputs here\n",
    "    first_derivative = # TODO: Implement the first derivative here\n",
    "    second_derivative = # TODO: Implement the second derivative here\n",
    "    \n",
    "    return model_outputs, first_derivative, second_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the compute_model_and_derivatives function\n",
    "torch.manual_seed(123)\n",
    "model = shoNN(1, 1, 32, 3)\n",
    "time_vector = torch.linspace(0, 1, steps=10).view(-1, 1).requires_grad_(True)\n",
    "model_outputs, first_derivative, second_derivative = compute_model_and_derivatives(\n",
    "    model, time_vector\n",
    ")\n",
    "\n",
    "# Check the shapes of the outputs\n",
    "assert (\n",
    "    model_outputs.shape == time_vector.shape\n",
    "), \"The shape of model_outputs is not correct.\"\n",
    "assert (\n",
    "    first_derivative.shape == time_vector.shape\n",
    "), \"The shape of first_derivative is not correct.\"\n",
    "assert (\n",
    "    second_derivative.shape == time_vector.shape\n",
    "), \"The shape of second_derivative is not correct.\"\n",
    "\n",
    "# Check the types of the outputs\n",
    "assert isinstance(\n",
    "    model_outputs, torch.Tensor\n",
    "), \"model_outputs should be a torch.Tensor.\"\n",
    "assert isinstance(\n",
    "    first_derivative, torch.Tensor\n",
    "), \"first_derivative should be a torch.Tensor.\"\n",
    "assert isinstance(\n",
    "    second_derivative, torch.Tensor\n",
    "), \"second_derivative should be a torch.Tensor.\"\n",
    "\n",
    "# Check the values of the outputs\n",
    "assert torch.allclose(\n",
    "    model_outputs, model(time_vector)\n",
    "), \"model_outputs should be the output of the model.\"\n",
    "assert torch.allclose(\n",
    "    first_derivative,\n",
    "    torch.autograd.grad(\n",
    "        model_outputs, time_vector, torch.ones_like(model_outputs), create_graph=True\n",
    "    )[0],\n",
    "), \"first_derivative should be the first derivative of the model outputs.\"\n",
    "assert torch.allclose(\n",
    "    second_derivative,\n",
    "    torch.autograd.grad(\n",
    "        first_derivative,\n",
    "        time_vector,\n",
    "        torch.ones_like(first_derivative),\n",
    "        create_graph=True,\n",
    "    )[0],\n",
    "), \"second_derivative should be the second derivative of the model outputs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can compute the derivatives, we define the loss function for the physics terms. We will use the mean squared error loss for the boundary term and the SHO term. We will then train the network with these loss functions and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def physics_loss(amplitude, first_derivative, second_derivative, frequency):\n",
    "    \"\"\"\n",
    "    This function should return the physics loss.\n",
    "    See the differential equations in mathematical form above.\n",
    "\n",
    "    Args:\n",
    "        amplitude: The amplitude of the SHO. x(t)\n",
    "        first_derivative: The first derivative of the amplitude. dx/dt\n",
    "        second_derivative: The second derivative of the amplitude. d^2x/dt^2\n",
    "        frequency: The frequency of the SHO. (omega)\n",
    "    \"\"\"\n",
    "    loss = # TODO: Implement the physics loss here\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case 1: When amplitude, first_derivative, second_derivative, and frequency are all zeros\n",
    "amplitude = torch.tensor([0.0])\n",
    "first_derivative = torch.tensor([0.0])\n",
    "second_derivative = torch.tensor([0.0])\n",
    "frequency = torch.tensor([0.0])\n",
    "expected_output = torch.tensor([0.0])\n",
    "assert (\n",
    "    physics_loss(amplitude, first_derivative, second_derivative, frequency)\n",
    "    == expected_output\n",
    ")\n",
    "\n",
    "# Test case 2: When amplitude, first_derivative, second_derivative, and frequency are all ones\n",
    "amplitude = torch.tensor([1.0])\n",
    "first_derivative = torch.tensor([1.0])\n",
    "second_derivative = torch.tensor([1.0])\n",
    "frequency = torch.tensor([1.0])\n",
    "expected_output = torch.tensor([4.0])\n",
    "assert (\n",
    "    physics_loss(amplitude, first_derivative, second_derivative, frequency)\n",
    "    == expected_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_initial_condition_loss(predicted_amplitude, predicted_first_derivative, expected_amplitude=1, expected_first_derivative=0):\n",
    "    \"\"\"\n",
    "    We want the initial position (t=0) to have an amplitude of 1 and the initial velocity to be 0.\n",
    "\n",
    "    Args:\n",
    "        predicted_amplitude: The predicted amplitude of the SHO at t=0.\n",
    "        predicted_first_derivative: The predicted first derivative of the amplitude at t=0.\n",
    "        expected_amplitude: The expected amplitude at t=0.\n",
    "        expected_first_derivative: The expected first derivative at t=0.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the initial condition loss here\n",
    "    initial_amplitude_loss = # TODO: Implement the initial amplitude loss here\n",
    "    initial_velocity_loss = # TODO: Implement the initial velocity loss here\n",
    "\n",
    "    # We will sum these two losses to get the total initial condition loss\n",
    "    loss = initial_amplitude_loss + initial_velocity_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test when predicted values match the expected values\n",
    "assert (\n",
    "    compute_initial_condition_loss(1, 0) == 0\n",
    "), \"Test failed: Expected loss is 0 when predicted values match the expected values\"\n",
    "\n",
    "# Test when predicted values do not match the expected values\n",
    "assert (\n",
    "    compute_initial_condition_loss(2, 1) == 2\n",
    "), \"Test failed: Expected loss is 2 when predicted values do not match the expected values\"\n",
    "\n",
    "# Test when expected values are different from the default values\n",
    "assert (\n",
    "    compute_initial_condition_loss(2, 1, 2, 1) == 0\n",
    "), \"Test failed: Expected loss is 0 when predicted values match the non-default expected values\"\n",
    "\n",
    "# Test with negative values\n",
    "assert (\n",
    "    compute_initial_condition_loss(-1, -1) == 5\n",
    "), \"Test failed: Expected loss is 5 when predicted values are negative\"\n",
    "\n",
    "# Test with floating point values\n",
    "assert (\n",
    "    compute_initial_condition_loss(1.5, 0.5) == 0.5\n",
    "), \"Test failed: Expected loss is 0.5 when predicted values are floating point numbers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "time_physics = (\n",
    "    torch.linspace(0, 1, 100).view(-1, 1).requires_grad_(True)\n",
    ")  # Generate some time values that we will run the SHO neaural network on\n",
    "\n",
    "initial_time = (\n",
    "    torch.tensor(0.0).view(-1, 1).requires_grad_(True)\n",
    ")  # providing initial time = 0 for initial conditions, x(t)=1, dx/dt=0\n",
    "torch.manual_seed(1)\n",
    "model = shoNN(1, 1, 32, 3)\n",
    "files = []\n",
    "sho_loss_weight = 1e-4\n",
    "initial_condition_loss_weight = 1e-4\n",
    "# frequency = nn.Parameter(torch.zeros(1, requires_grad=True))\n",
    "frequency = 30\n",
    "\n",
    "# optimizer = torch.optim.Adam(list(model.parameters())+[frequency],lr=1e-2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "losses_physics = []\n",
    "losses_sho = []\n",
    "losses_initial_condition = []\n",
    "\n",
    "for i in range(60000):\n",
    "    optimizer.zero_grad()\n",
    "    sho_predicted = model(time_samples)\n",
    "    loss1 = torch.mean((sho_predicted - sho_samples) ** 2)  # use mean squared error\n",
    "\n",
    "    # compute the physics loss not on the boudnary or initial condition\n",
    "    sho_predicted_physics, dtsho_predicted_physics, d2tsho_predicted_physics = (\n",
    "        compute_model_and_derivatives(model, time_physics)\n",
    "    )\n",
    "    sho_loss = physics_loss(\n",
    "        sho_predicted_physics,\n",
    "        dtsho_predicted_physics,\n",
    "        d2tsho_predicted_physics,\n",
    "        frequency,\n",
    "    )\n",
    "\n",
    "    # compute the initial condition loss\n",
    "    initial_sho_x, initial_sho_dt, initial_sho_d2t = compute_model_and_derivatives(\n",
    "        model, initial_time\n",
    "    )\n",
    "    initial_condition_loss = compute_initial_condition_loss(\n",
    "        initial_sho_x, initial_sho_dt\n",
    "    )\n",
    "\n",
    "    loss = (\n",
    "        loss1\n",
    "        + sho_loss_weight * sho_loss\n",
    "        + initial_condition_loss_weight * initial_condition_loss\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses_sho.append(sho_loss.item())\n",
    "    losses_initial_condition.append(initial_condition_loss.item())\n",
    "    losses_physics.append(loss.item())\n",
    "\n",
    "    if (i + 1) % 150 == 0:\n",
    "        sho_predicted = model(time).detach()\n",
    "        time_p = time_physics.detach()\n",
    "\n",
    "        plot_result(\n",
    "            time,\n",
    "            sho_analytic_solution,\n",
    "            time_samples,\n",
    "            sho_samples,\n",
    "            sho_predicted,\n",
    "            time_p,\n",
    "        )\n",
    "        file = \"../plots/nn_physics_%.8i.png\" % (i + 1)\n",
    "        plt.savefig(\n",
    "            file, bbox_inches=\"tight\", pad_inches=0.1, dpi=100, facecolor=\"white\"\n",
    "        )\n",
    "        files.append(file)\n",
    "\n",
    "        if (i + 1) % 6000 == 0:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(\"all\")\n",
    "\n",
    "plt.plot(losses_physics, label=\"Physics loss\")\n",
    "plt.plot(losses_sho, label=\"SHO loss\")\n",
    "plt.plot(losses_initial_condition, label=\"Initial condition loss\")\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "save_gif_PIL(\"physics_nn.gif\", files, fps=20, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
